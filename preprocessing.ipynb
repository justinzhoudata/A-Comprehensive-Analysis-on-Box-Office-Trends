{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">**Movie Data Preprocessing**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">**Part 1:** Webscraping</font>\n",
    "\n",
    "In this part, I focused on extracting valuable data from the \"IMDb Top 250\" webpage. To achieve this, I first used the requests library to download the webpage's HTML content. Then, I parsed the HTML content using the BeautifulSoup library, which enabled me to extract the necessary information from the webpage's HTML tags. I extracted the movie titles, years of release, and IMDb ratings for each movie by selecting the appropriate HTML tags using the select() method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import math\n",
    " \n",
    " \n",
    "# Downloading imdb top 250 movie's data\n",
    "url = 'http://www.imdb.com/chart/top'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "movies = soup.select('td.titleColumn')\n",
    "ratings = [b.attrs.get('data-value')\n",
    "        for b in soup.select('td.posterColumn span[name=ir]')]\n",
    " \n",
    "# create an empty dataframe for storing\n",
    "# movie information\n",
    "imdbdf = pd.DataFrame()\n",
    " \n",
    "# Iterating over movies to extract\n",
    "# each movie's details\n",
    "for index in range(0, len(movies)):\n",
    "     \n",
    "    # Separating movie into: 'place',\n",
    "    # 'title', 'year'\n",
    "    movie_string = movies[index].get_text()\n",
    "    movie = (' '.join(movie_string.split()).replace('.', ''))\n",
    "    movie_title = movie[len(str(index))+1:-7]\n",
    "    year = re.search('\\((.*?)\\)', movie_string).group(1)\n",
    "    place = movie[:len(str(index))-(len(movie))]\n",
    "    data = {\"place\": place,\n",
    "            \"movie_title\": movie_title,\n",
    "            \"imdb_score\": ratings[index],\n",
    "            \"year\": year,\n",
    "            }\n",
    "    imdbdf = imdbdf.append(data, ignore_index=True)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">**Part 2:** Rotten Tomatoes Dataset</font>\n",
    "\n",
    "In this analysis, the dataset used was obtained from Kaggle, which contains information on top-rated movies from Rotten Tomatoes: https://www.kaggle.com/datasets/thedevastator/rotten-tomatoes-top-movies-ratings-and-technical. To prepare the dataset for analysis, irrelevant variables were dropped. The variables that were deemed relevant for analysis, such as the rating, genre, runtime, and box office variables, were reformatted to ensure they are better suited for the regression analysis.\n",
    "\n",
    "To further enhance the analysis, a sentiment score was calculated for each movie synopsis. This was done using a lexicon-based approach with the VADER tool. This tool assigns a sentiment score to text based on a pre-built lexicon that contains positive and negative words. The sentiment score ranges from -1 (most negative) to 1 (most positive) and 0 represents a neutral score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a pandas dataframe\n",
    "df = pd.read_csv('tomatoes.csv')\n",
    "\n",
    "# Drop the specified columns\n",
    "columns_to_drop = ['consensus', 'total_ratings', 'original_language', \n",
    "                   'release_date_(streaming)', 'production_co', \n",
    "                   'sound_mix', 'aspect_ratio', 'link', 'producer',\n",
    "                  'release_date_(theaters)',]\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "\n",
    "#Remove rating explanation from rating column\n",
    "df['rating'] = df['rating'].str.replace(r'\\(.*\\)', '')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['history', 'fantasy', 'other', 'comedy', 'sports and fitness', 'western', 'action', 'kids and family', 'anime', 'biography', 'adventure', 'animation', 'horror', 'music', 'crime', 'sci fi', 'romance', 'musical', 'documentary', 'mystery and thriller', 'gay and lesbian', 'war', 'drama']\n"
     ]
    }
   ],
   "source": [
    "# Filter out rows with missing or null values in the \"genre\" column\n",
    "genrelist = df.loc[~pd.isna(df['genre']), 'genre'].tolist()\n",
    "\n",
    "\n",
    "# Split each string into a list of words, and combine all the lists\n",
    "all_words = [word for sublist in [string.split(',') for string in genrelist] for word in sublist]\n",
    "\n",
    "all_words = [s.strip() for s in all_words]\n",
    "\n",
    "# Get the unique words from the combined list\n",
    "unique_words = list(set(all_words))\n",
    "\n",
    "# Print the resulting list of unique words\n",
    "print(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a new dataframe with dummies\n",
    "dummies_df = pd.get_dummies(df['genre'].apply(pd.Series).stack()).sum(level=0)\n",
    "\n",
    "# reindex the new dataframe with the unique genres\n",
    "dummies_df = dummies_df.reindex(columns=unique_words, fill_value=0)\n",
    "\n",
    "# concatenate the original dataframe with the new dummies dataframe\n",
    "df = pd.concat([df, dummies_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rating'] = df['rating'].str.strip()\n",
    "# create a new dataframe with dummies\n",
    "ratingdummies = pd.get_dummies(df['rating'])\n",
    "\n",
    "# concatenate the original dataframe with the new dummies dataframe\n",
    "df = pd.concat([df, ratingdummies], axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting runtime to minutes\n",
    "\n",
    "# Define a function to convert the 'runtime' values to total minutes\n",
    "\n",
    "def convert_to_minutes(runtime):\n",
    "    if isinstance(runtime, str):\n",
    "        if 'h ' in runtime:\n",
    "            hours, minutes = runtime.split('h ')\n",
    "            return int(hours) * 60 + int(minutes[:-1])\n",
    "        else:\n",
    "            return int(runtime[:-1])\n",
    "    else:\n",
    "        return math.nan\n",
    "\n",
    "\n",
    "\n",
    "# Apply the function to the 'runtime' column to create a new 'total_minutes' column\n",
    "df['runtime'] = df['runtime'].apply(convert_to_minutes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to convert the 'box_office_(gross_usa)' values to numbers\n",
    "def convert_to_numbers(box_office):\n",
    "    if isinstance(box_office, str):\n",
    "        if box_office[-1] == 'M':\n",
    "            return float(box_office[1:-1]) * 1000000\n",
    "        elif box_office[-1] == 'K':\n",
    "            return float(box_office[1:-1]) * 1000\n",
    "    return math.nan\n",
    "\n",
    "\n",
    "# Apply the function to the 'box_office_(gross_usa)' column to create a new 'boxoffice' column\n",
    "df['boxoffice'] = df['box_office_(gross_usa)'].apply(convert_to_numbers)\n",
    "\n",
    "# Convert the 'boxoffice' column to float\n",
    "df['boxoffice'] = pd.to_numeric(df['boxoffice'])\n",
    "\n",
    "# Drop the original 'box_office_(gross_usa)' column\n",
    "df.drop('box_office_(gross_usa)', axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Drop rows with NaN or float in 'synopsis' column\n",
    "df = df.dropna(subset=['synopsis'])\n",
    "df = df[~df['synopsis'].apply(lambda x: isinstance(x, float))]\n",
    "\n",
    "# Instantiate the analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function to calculate the sentiment score for each synopsis\n",
    "def get_sentiment_score(text):\n",
    "    score = analyzer.polarity_scores(text)\n",
    "    return score['compound']\n",
    "\n",
    "# Apply the function to the 'synopsis' column to create a new 'sentiment_score' column\n",
    "df['sentiment_score'] = df['synopsis'].apply(get_sentiment_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">**Part 3:** Export Data</font>\n",
    "\n",
    "Exporting data to a .csv file to analyze in SQL and Tableau!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export imdbdf to a CSV file\n",
    "imdbdf.to_csv('processed_imdb.csv', index=False)\n",
    "\n",
    "# Export df to a CSV file\n",
    "df.to_csv('processed_tomato.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
